{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T19:16:54.929607Z",
     "start_time": "2018-01-22T19:16:52.431309Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from gensim import corpora\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models.wrappers.fasttext import FastText as FT_wrapper\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from random import shuffle\n",
    "import time\n",
    "import re\n",
    "import pylab as pl\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T19:21:10.128634Z",
     "start_time": "2018-01-22T19:17:03.658824Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(\"../w2v/model/wiki-news-300d-1M.vec\")\n",
    "# w2v = KeyedVectors.load(\"../w2v/model/fasttext_w2v_vector_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_url(main_url):\n",
    "    resp = requests.get(main_url)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    urls = []\n",
    "    links = soup.find_all('a')\n",
    "    for url in links:\n",
    "        try:\n",
    "            url = url.attrs['href']\n",
    "            if len(url) > 5:\n",
    "                urls.append(url)\n",
    "        except:\n",
    "            pass\n",
    "    return urls\n",
    "\n",
    "def get_texts_from_resp(resp):\n",
    "    # parse the web response\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    # find and filter texts\n",
    "    print(\"These are texts under\", resp.url)\n",
    "    texts = soup.find_all('p')\n",
    "    print(\"number of items grabed are\", len(texts))\n",
    "    texts = [text for text in texts if len(text.text) > 100]\n",
    "    print(\"number of items after filtering\", len(texts))\n",
    "    # output texts\n",
    "    for text in texts:\n",
    "        #print(text.text)\n",
    "        yield text.text\n",
    "\n",
    "def url_is_valid(url):\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        assert resp.status_code == 200\n",
    "        return resp\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def url_compare(url_target, url_income):\n",
    "    n_same_letter = 0.0\n",
    "    # delete all http or https \n",
    "    if url_target[4] == 's':\n",
    "        url_target = url_target[5:]\n",
    "    else:\n",
    "        url_target = url_target[4:]\n",
    "    if url_income[4] == 's':\n",
    "        url_income = url_income[5:]\n",
    "    else:\n",
    "        url_income = url_income[4:]\n",
    "    # check similarity\n",
    "    min_len = min(len(url_target), len(url_income))\n",
    "    for i in range(min_len-1):\n",
    "        if url_target[i] == url_income[i]:\n",
    "            n_same_letter += 1\n",
    "        else:\n",
    "            break\n",
    "    return n_same_letter\n",
    "\n",
    "def get_text_from_url_with_check(url, main_url):\n",
    "    resp = url_is_valid(url)\n",
    "    if not resp:\n",
    "        url = main_url + url\n",
    "        resp = url_is_valid(url)\n",
    "        if not resp:\n",
    "            print(\"url:\", url, \"invalid\")\n",
    "            return []\n",
    "    # double check if the url is visited\n",
    "    if resp.url != url: # meaning its redirected\n",
    "        print('the url is redirected, try https\\n')\n",
    "        # try https\n",
    "        url = url[:4] + 's' + url[4:]\n",
    "        resp = url_is_valid(url)\n",
    "        if resp:\n",
    "            if resp.url == url:\n",
    "                print('try succeeded')\n",
    "        else:\n",
    "            return []\n",
    "    # check if url is the child or sibling of main_url\n",
    "    if url_compare(main_url, resp.url) < 10: # to avoid http://www.\n",
    "        print('\\nurl:', resp.url, 'might be irrelevent to', main_url, 'quit visiting\\n')\n",
    "        return []\n",
    "    text_data = []\n",
    "    for text in get_texts_from_resp(resp):\n",
    "        text_data.append(text)\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Searcher():\n",
    "    def __init__(self, w2v=None, database=None):\n",
    "        # load w2v modle\n",
    "        if w2v is None:\n",
    "            print(\"start loading w2v, this might take a while\")\n",
    "            self._w2v = KeyedVectors.load_word2vec_format(\"../w2v/model/wiki-news-300d-1M.vec\")\n",
    "        else:\n",
    "            self._w2v = w2v\n",
    "        \n",
    "        # get and process database\n",
    "        if database is None:\n",
    "            # if no crawled database given\n",
    "            # load the dataset : including only each company's name, url and summary\n",
    "            self._database = pd.read_csv(\"../input/InvestData_2017-Nov-22_0101.csv\").iloc[:, [1, 5, 6]]\n",
    "            self.crawl_database()\n",
    "        else:\n",
    "            self._database = database\n",
    "        self.process_database()\n",
    "        \n",
    "    def process_database(self):\n",
    "        # 1: company name, 5: company website, 6: company manual desc\n",
    "        raw_texts = []\n",
    "        # preprocess all the text data and remove any row without any useful data, and segment each word\n",
    "        drop_list = []\n",
    "        for row in self._database.itertuples():\n",
    "            if not type(row[1]) is str or (not type(row[2]) is str and not type(row[3]) is str):\n",
    "                # check if the row has data\n",
    "                drop_list.append(row[0])\n",
    "            else:\n",
    "                # process text data of both manually summarized or crawled data\n",
    "                tmp_text = []\n",
    "                for col in [2, 3]:\n",
    "                    real_col = col - 1\n",
    "                    text = row[col]\n",
    "                    if type(text) is str:\n",
    "                        text = self.word_tokenize_string(text)\n",
    "                        self._database.iloc[row[0], real_col] = text\n",
    "                        tmp_text.append(text)\n",
    "                # merge texts of same company\n",
    "                tmp_text = '    '.join(tmp_text)\n",
    "                raw_texts.append(tmp_text)\n",
    "                \n",
    "        # drop all the rows that do not have essential data\n",
    "        self._database.drop(drop_list, inplace=True)\n",
    "        # create similarity col for similarity search use\n",
    "        self._database = self._database.assign(similarity=np.zeros(len(self._database)))\n",
    "        \n",
    "        # use the raw_texts to generate tfidf model\n",
    "        self._tfidf, self._dictionary = self.get_tfidf_and_dictionary(raw_texts)\n",
    "        \n",
    "    def crawl_database(self):\n",
    "        for row in self._database.itertuples():\n",
    "            if not (not type(row[1]) is str or (not type(row[2]) is str and not type(row[3]) is str)):\n",
    "                # process each website and replace web address with texts crawled\n",
    "                url = row[2]\n",
    "                texts = self.get_text_from_url_and_its_children(url)\n",
    "                if not texts:\n",
    "                    # if cannot access url, replace url with Nan\n",
    "                    self._database.iloc[row[0], 1] = np.nan\n",
    "                else:\n",
    "                    # replace the url with the crawled texts\n",
    "                    texts = '   '.join(texts)\n",
    "                    self._database.iloc[row[0], 1] = texts\n",
    "        \n",
    "    def save_database(self):\n",
    "        self._database.to_csv('crawled_database.csv')\n",
    "        print(\"database save successful\")\n",
    "    \n",
    "    def update_similarity(self, input_text, col=1):\n",
    "        # get input text vector\n",
    "        input_text_vector = self._get_doc_vector(input_text)\n",
    "        i = 0\n",
    "        for row in database.itertuples():\n",
    "            row_text_vector = self._get_doc_vector(row[col+1])\n",
    "            similarity = input_text_vector.dot(row_text_vector)\n",
    "            database.iloc[i, -1] = similarity\n",
    "            i += 1\n",
    "        self._database = self._database.sort_values(by='similarity', ascending=False)\n",
    "        return database\n",
    "\n",
    "    def get_doc_vector(self, text):\n",
    "        tokens = list(self._dictionary.token2id)\n",
    "        # convert any unknown word to known word\n",
    "        new_text = []\n",
    "        for word in text.split():\n",
    "            if word in tokens:\n",
    "                new_text.append(word)\n",
    "            elif word in w2v: # replace the unknow word with the most similar word in tokens of dictionary\n",
    "                new_text.append(self._w2v.most_similar_to_given(word_list=tokens, w1=word))\n",
    "\n",
    "        # start to calculate vector using tfidf weighted word vector sum\n",
    "        # get tfidf weight\n",
    "        tokenized_text = [self._dictionary.doc2bow(new_text)]\n",
    "        tfidf_text = self._tfidf[tokenized_text][0]\n",
    "        # sum weighted word vectors\n",
    "        sum_vector = self._w2v['happy'] * 0 # get the size of the word vector\n",
    "        for word_id, weight in tfidf_text:\n",
    "            word = self._dictionary[word_id]\n",
    "            sum_vector += self._w2v[word] * weight\n",
    "        sum_vector /= np.sqrt(sum_vector.dot(sum_vector)) # normalize the vector\n",
    "\n",
    "        return sum_vector\n",
    "    \n",
    "    def word_tokenize_string(self, text):\n",
    "        stop_words = get_stop_words('en') # get too frequent word\n",
    "        text = text.replace('\\r', ' ').replace('\\n', ' ') # remove symbols\n",
    "        text = re.sub(r\"http\\S+\", \"\", text) # remove urls\n",
    "        # remove any word that present too frequently or cannot be converted to word vector\n",
    "        text = [word for sent in sent_tokenize(text.lower()) for word in word_tokenize(sent) \\\n",
    "                if not word in stop_words and word in self._w2v]\n",
    "        return ' '.join(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tfidf_and_dictionary(texts):\n",
    "        # get dictionary of texts\n",
    "        texts = [text.split() for text in texts]\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "        # get tfidf ranking model\n",
    "        tokenized_texts = [dictionary.doc2bow(text) for text in texts]\n",
    "        tfidf = models.TfidfModel(tokenized_texts)\n",
    "\n",
    "        return tfidf, dictionary\n",
    "    \n",
    "    def get_text_from_url_and_its_children(self, main_url):\n",
    "        print(\"starting to crawl main url: \", main_url)\n",
    "        # check validity of main_url\n",
    "        resp = url_is_valid(main_url)\n",
    "        if not resp:\n",
    "            print(\"main_url is not valid\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\nstarting to crawl all its children\")\n",
    "        # grab all urls in this web page\n",
    "        urls = [main_url]\n",
    "        urls.extend(get_urls_from_url(main_url))\n",
    "        urls = list(set(urls)) # remove duplicated urls\n",
    "        print(\"\\n\\nthese are the children links we crawled\")\n",
    "        print(urls, \"\\n\")\n",
    "        # grab all texts in each urls asynchronously\n",
    "        # argmumentize urls\n",
    "        urls = [(url, main_url) for url in urls]\n",
    "        with multiprocessing.Pool(processes=24) as pool:\n",
    "            text_data = pool.starmap(get_text_from_url_with_check, urls) \n",
    "            # try terminating hung jobs\n",
    "        text_data = [text for text in text_data if len(text_data) > 0] # remove empty returns\n",
    "        text_data = [text for text_list in text_data for text in text_list] # get list elements to str\n",
    "        return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to crawl main url:  http://www.aecfafrica.org/\n",
      "\n",
      "starting to crawl all its children\n",
      "\n",
      "\n",
      "these are the children links we crawled\n",
      "['/contact-us', '/contact', '/the-aecf-board', '/about-us/who-we-are', '/portfolio/renewable-energy', '/about-us/our-history', '/about-us/strategic-partners', '/portfolio/aecf-connect', 'tel:+254703033394', '\\n\\n\\n\\n\\n/portfolio/renewable-energy\\n\\n\\n', '/about-us/funding-partners', '/portfolio/competitions', '/portfolio/agribusiness', '/about-us/work-with-us', 'http://www.aecfafrica.org/', '/media-centre/videos', 'http://www.parioagency.com', '/news/sida_signs_five_year_agreement_with_the_AECF', '\\n\\n\\n\\n\\n/portfolio/agribusiness\\n\\n\\n', 'https://www.facebook.com/AECFAfrica/', '\\n\\n\\n\\n\\n/about-us/who-we-are\\n\\n\\n', '/knowledge-hub', 'https://www.linkedin.com/company/africa-enterprise-challenge-fund', 'mailto:info@aecfafrica.org', '/portfolio/overview', '/media-centre/blog', '/impact', '\\n\\n\\n\\n\\n/about-us/funding-partners\\n\\n\\n', 'https://twitter.com/AecfAfrica', '/the-aecf-management', '/media-centre/news', 'tel:+254203675394'] \n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "the url is redirected, try https\n",
      "\n",
      "\n",
      "the url is redirected, try https\n",
      "the url is redirected, try https\n",
      "\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "the url is redirected, try https\n",
      "\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "These are texts under https://www.aecfafrica.org/news/sida_signs_five_year_agreement_with_the_AECF\n",
      "number of items grabed are 17\n",
      "number of items after filtering 13\n",
      "\n",
      "url: https://www.facebook.com/AECFAfrica/ might be irrelevent to http://www.aecfafrica.org/ quit visiting\n",
      "\n",
      "These are texts under https://www.aecfafrica.org/portfolio/renewable-energy\n",
      "number of items grabed are 5\n",
      "number of items after filtering 2\n",
      "try succeeded\n",
      "These are texts under https://www.aecfafrica.org/\n",
      "number of items grabed are 9\n",
      "number of items after filtering 5\n",
      "These are texts under https://www.aecfafrica.org/portfolio/agribusiness\n",
      "number of items grabed are 9\n",
      "number of items after filtering 6\n",
      "the url is redirected, try https\n",
      "\n",
      "These are texts under https://www.aecfafrica.org/about-us/strategic-partners\n",
      "number of items grabed are 4\n",
      "number of items after filtering 2\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "\n",
      "url: https://twitter.com/AecfAfrica might be irrelevent to http://www.aecfafrica.org/ quit visiting\n",
      "\n",
      "the url is redirected, try https\n",
      "\n",
      "These are texts under https://www.aecfafrica.org/knowledge-hub\n",
      "number of items grabed are 9\n",
      "number of items after filtering 5\n",
      "These are texts under https://www.aecfafrica.org/about-us/our-history\n",
      "number of items grabed are 12\n",
      "number of items after filtering 8\n",
      "These are texts under https://www.aecfafrica.org/media-centre/videos\n",
      "number of items grabed are 7\n",
      "number of items after filtering 0\n",
      "These are texts under https://www.aecfafrica.org/media-centre/blog\n",
      "number of items grabed are 6\n",
      "number of items after filtering 0\n",
      "These are texts under https://www.aecfafrica.org/contact\n",
      "number of items grabed are 1\n",
      "number of items after filtering 0\n",
      "the url is redirected, try https\n",
      "\n",
      "These are texts under https://www.aecfafrica.org/about-us/funding-partners\n",
      "number of items grabed are 19\n",
      "number of items after filtering 10\n",
      "These are texts under https://www.aecfafrica.org/portfolio/aecf-connect\n",
      "These are texts under https://www.aecfafrica.org/the-aecf-board\n",
      "number of items grabed are 8\n",
      "number of items grabed are 20\n",
      "number of items after filtering 5\n",
      "number of items after filtering 19\n",
      "These are texts under https://www.aecfafrica.org/portfolio/competitions\n",
      "number of items grabed are 34\n",
      "number of items after filtering 25\n",
      "These are texts under https://www.aecfafrica.org/about-us/who-we-are\n",
      "number of items grabed are 7\n",
      "number of items after filtering 3\n",
      "These are texts under https://www.aecfafrica.org/contact-us\n",
      "number of items grabed are 4\n",
      "number of items after filtering 0\n",
      "These are texts under https://www.aecfafrica.org/about-us/work-with-us\n",
      "number of items grabed are 9\n",
      "number of items after filtering 3\n",
      "These are texts under https://www.aecfafrica.org/impact\n",
      "number of items grabed are 3\n",
      "number of items after filtering 1\n",
      "These are texts under https://www.aecfafrica.org/portfolio/overview\n",
      "number of items grabed are 1\n",
      "number of items after filtering 0\n",
      "These are texts under https://www.aecfafrica.org/media-centre/news\n",
      "number of items grabed are 31\n",
      "number of items after filtering 13\n",
      "These are texts under https://www.aecfafrica.org/the-aecf-management\n",
      "number of items grabed are 16\n",
      "number of items after filtering 6\n"
     ]
    }
   ],
   "source": [
    "searcher = Searcher(w2v=w2v)\n",
    "# there might be main_url + url is not valid, because url and main_url has overlaps, or main_url is not the root\n",
    "# must provide root url, or \n",
    "# use overlaps to do intelligent main_url + url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to crawl main url:  http://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments\n",
      "\n",
      "starting to crawl all its children\n",
      "\n",
      "\n",
      "these are the children links we crawled\n",
      "['/math/matrix-vector-array-numpy', '/math/statistics', 'https://docs.python.org/3/library/multiprocessing.html', '/home/help', '/www/download-webpage', '/strings', '/argparse', '/plot/matplotlib', '/loops/if-else', '/math/types', '/debugging', '/home/download-install', '/error-handling/raise-type-checking', '/multiprocessing_map', '/packages', 'http://sites.google.com', '/sytem/environment', '/error/taberror', '/loops/list-comprehension', '/print-write-format/error', '/www/cgi-script', '/data-structures/dictionary', '/sytem', '/file-operations/files-read-write', '/packages/create-modules', '/packages/python-script', 'https://docs.python.org/3.4/library/functools.html#functools.partial', '/strings/safe-characters', 'http://python.omics.wiki/', '/math/dataframe', '/system/app/pages/recentChanges', '/loops', 'http://sites.google.com/site/python3tutorial/system/app/pages/reportAbuse', '/data-structures', '/function-definition', '/packages/pip-install', '/data-structures/lists', '/strings/comparison', '/plot/barplot', '/error-handling/check-version', '/system/app/pages/sitemap/hierarchy', '/strings/str-vs-repr', 'https://docs.python.org/3/library/functools.html#functools.partial', '/file-operations/file-commands', '/file-operations', '/biopython/examples', '/biopython', '/argparse/valueerror', '/math/machine-learning', '/error-handling', 'javascript:;', '/print-write-format', '/loops/for-while-loops', '/packages/import-module', 'https://www.google.com/a/UniversalLogin?continue=http://sites.google.com/site/python3tutorial/multiprocessing_map/multiprocessing_partial_function_multiple_arguments&service=jotspot', '/sytem/subprocess', '/plot/colors', 'http://pool.map/', 'http://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments', '/error', '/www/graphics', '/biopython/install'] \n",
      "\n",
      "url: http://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments/math/matrix-vector-array-numpy invalid\n",
      "url: http://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments/math/statistics invalid\n",
      "\n",
      "url:\n",
      " https://docs.python.org/3/library/multiprocessing.html might be irrelevent to http://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments quit visiting\n",
      "url: http://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments/home/help invalid\n",
      "url: http://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments/www/download-webpage invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-191:\n",
      "Process ForkPoolWorker-190:\n",
      "Process ForkPoolWorker-192:\n",
      "Process ForkPoolWorker-194:\n",
      "Process ForkPoolWorker-180:\n",
      "Process ForkPoolWorker-183:\n",
      "Process ForkPoolWorker-193:\n",
      "Process ForkPoolWorker-196:\n",
      "Process ForkPoolWorker-181:\n",
      "Process ForkPoolWorker-195:\n",
      "Process ForkPoolWorker-179:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-182:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-189:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "Process ForkPoolWorker-185:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Process ForkPoolWorker-174:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkPoolWorker-177:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "Process ForkPoolWorker-186:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkPoolWorker-187:\n",
      "Process ForkPoolWorker-176:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-188:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Process ForkPoolWorker-175:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-184:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-173:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-178:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/yuze/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/queues.py\", line 337, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9b9a2aa2b953>\u001b[0m in \u001b[0;36mget_text_from_url_and_its_children\u001b[0;34m(self, main_url)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_from_url_with_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# remove empty returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    273\u001b[0m         '''\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e7fd1c5e23b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_from_url_and_its_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-9b9a2aa2b953>\u001b[0m in \u001b[0;36mget_text_from_url_and_its_children\u001b[0;34m(self, main_url)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_url\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_from_url_with_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# remove empty returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_data\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get list elements to str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[1;32m    185\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'helping task handler/workers to finish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_help_stuff_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_help_stuff_finish\u001b[0;34m(inqueue, task_handler, size)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# task_handler may be blocked trying to put items on inqueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'removing tasks from inqueue until task handler finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "url = 'http://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments'\n",
    "searcher.get_text_from_url_and_its_children(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T19:17:02.941319Z",
     "start_time": "2018-01-22T19:17:02.936963Z"
    }
   },
   "outputs": [],
   "source": [
    "def rescue_code(function):\n",
    "    import inspect\n",
    "    get_ipython().set_next_input(\"\".join(inspect.getsourcelines(function)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T19:21:48.699686Z",
     "start_time": "2018-01-22T19:21:48.696223Z"
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"new start up aiming at low income customers, dedicated in green energy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T19:24:06.347593Z",
     "start_time": "2018-01-22T19:24:06.173975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fund for Developing',\n",
       " '. invests enterprises low income countries promote business development contribute economic growth poverty alleviation . ’ s geographic focus eastern southern africa , well selected countries asia central america . focuses supporting small medium sized companies .',\n",
       " nan,\n",
       " 0.87590879201889038]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_output = update_similarity(w2v, dictionary, tfidf, input_text, database)\n",
    "list(search_output.iloc[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T19:05:26.850146Z",
     "start_time": "2017-12-04T19:05:26.718942Z"
    }
   },
   "outputs": [],
   "source": [
    "vec1 = get_doc_vector('a startup that dedicate to green energy', w2v, dictionary, tfidf)\n",
    "vec2 = get_doc_vector('business regrading green energy', w2v, dictionary, tfidf)\n",
    "vec3 = get_doc_vector('companies specificly support low-income people', w2v, dictionary, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T19:05:33.333190Z",
     "start_time": "2017-12-04T19:05:33.327018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83994347"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1.dot(vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T18:34:27.058690Z",
     "start_time": "2017-12-04T18:34:27.050733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01663876, -0.00337446, -0.02395986, -0.04154515, -0.10667257,\n",
       "        0.01662513, -0.01009543,  0.06161033,  0.05003018, -0.06181473,\n",
       "       -0.02179295, -0.01145902, -0.01362746, -0.03405415,  0.00218448,\n",
       "        0.00767476, -0.01452028, -0.0018257 ,  0.07659619, -0.04767921,\n",
       "       -0.04837132, -0.02310556, -0.05546847, -0.00884663, -0.01575219,\n",
       "        0.00686825, -0.02110741,  0.02846674, -0.00410359,  0.0267566 ,\n",
       "        0.0219034 ,  0.01259031, -0.00153076,  0.0267643 ,  0.04200932,\n",
       "       -0.05939888,  0.02810077, -0.01088257,  0.02167377,  0.05022477,\n",
       "        0.00617732, -0.01705034,  0.04542316,  0.08962091, -0.01164754,\n",
       "        0.0392498 , -0.02558717, -0.0163672 ,  0.06803226, -0.01166763,\n",
       "        0.01418332,  0.04633227, -0.00154001,  0.00630221, -0.05789408,\n",
       "        0.02684514,  0.03213207,  0.02908244, -0.05910822,  0.01098826,\n",
       "       -0.04702628, -0.01257465,  0.02256784, -0.01471028], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-04T18:34:31.917415Z",
     "start_time": "2017-12-04T18:34:31.910376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01663876, -0.00337446, -0.02395986, -0.04154515, -0.10667257,\n",
       "        0.01662513, -0.01009543,  0.06161033,  0.05003018, -0.06181473,\n",
       "       -0.02179295, -0.01145902, -0.01362746, -0.03405415,  0.00218448,\n",
       "        0.00767476, -0.01452028, -0.0018257 ,  0.07659619, -0.04767921,\n",
       "       -0.04837132, -0.02310556, -0.05546847, -0.00884663, -0.01575219,\n",
       "        0.00686825, -0.02110741,  0.02846674, -0.00410359,  0.0267566 ,\n",
       "        0.0219034 ,  0.01259031, -0.00153076,  0.0267643 ,  0.04200932,\n",
       "       -0.05939888,  0.02810077, -0.01088257,  0.02167377,  0.05022477,\n",
       "        0.00617732, -0.01705034,  0.04542316,  0.08962091, -0.01164754,\n",
       "        0.0392498 , -0.02558717, -0.0163672 ,  0.06803226, -0.01166763,\n",
       "        0.01418332,  0.04633227, -0.00154001,  0.00630221, -0.05789408,\n",
       "        0.02684514,  0.03213207,  0.02908244, -0.05910822,  0.01098826,\n",
       "       -0.04702628, -0.01257465,  0.02256784, -0.01471028], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1410px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
