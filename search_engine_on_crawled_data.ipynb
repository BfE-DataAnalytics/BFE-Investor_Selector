{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T19:16:54.929607Z",
     "start_time": "2018-01-22T19:16:52.431309Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from gensim import corpora\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models.wrappers.fasttext import FastText as FT_wrapper\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from random import shuffle\n",
    "import time\n",
    "import re\n",
    "import pylab as pl\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T19:21:10.128634Z",
     "start_time": "2018-01-22T19:17:03.658824Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(\"../w2v/model/wiki-news-300d-1M.vec\")\n",
    "# w2v = KeyedVectors.load(\"../w2v/model/fasttext_w2v_vector_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_url(main_url):\n",
    "    resp = requests.get(main_url)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    urls = []\n",
    "    links = soup.find_all('a')\n",
    "    for url in links:\n",
    "        try:\n",
    "            url = url.attrs['href']\n",
    "            if len(url) > 5:\n",
    "                urls.append(url)\n",
    "        except:\n",
    "            pass\n",
    "    return urls\n",
    "\n",
    "def get_texts_from_resp(resp):\n",
    "    # parse the web response\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    # find and filter texts\n",
    "    print(\"These are texts under\", resp.url)\n",
    "    texts = soup.find_all('p')\n",
    "    print(\"number of items grabed are\", len(texts))\n",
    "    texts = [text for text in texts if len(text.text) > 100]\n",
    "    print(\"number of items after filtering\", len(texts))\n",
    "    # output texts\n",
    "    for text in texts:\n",
    "        #print(text.text)\n",
    "        yield text.text\n",
    "\n",
    "def url_is_valid(url):\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        assert resp.status_code == 200\n",
    "        return resp\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def url_compare(url_target, url_income):\n",
    "    n_same_letter = 0.0\n",
    "    # delete all http or https \n",
    "    if url_target[4] == 's':\n",
    "        url_target = url_target[5:]\n",
    "    else:\n",
    "        url_target = url_target[4:]\n",
    "    if url_income[4] == 's':\n",
    "        url_income = url_income[5:]\n",
    "    else:\n",
    "        url_income = url_income[4:]\n",
    "    # check similarity\n",
    "    min_len = min(len(url_target), len(url_income))\n",
    "    for i in range(min_len-1):\n",
    "        if url_target[i] == url_income[i]:\n",
    "            n_same_letter += 1\n",
    "        else:\n",
    "            break\n",
    "    return n_same_letter\n",
    "\n",
    "def get_text_from_url_with_check(url, main_url):\n",
    "    resp = url_is_valid(url)\n",
    "    if not resp:\n",
    "        url = main_url + url\n",
    "        resp = url_is_valid(url)\n",
    "        if not resp:\n",
    "            print(\"url:\", url, \"invalid\")\n",
    "            return []\n",
    "    # double check if the url is visited\n",
    "    if resp.url != url: # meaning its redirected\n",
    "        print('the url is redirected, try https\\n')\n",
    "        # try https\n",
    "        url = url[:4] + 's' + url[4:]\n",
    "        resp = url_is_valid(url)\n",
    "        if resp:\n",
    "            if resp.url == url:\n",
    "                print('try succeeded')\n",
    "        else:\n",
    "            return []\n",
    "    # check if url is the child or sibling of main_url\n",
    "    if url_compare(main_url, resp.url) < 10: # to avoid http://www.\n",
    "        print('\\nurl:', resp.url, 'might be irrelevent to', main_url, 'quit visiting\\n')\n",
    "        return []\n",
    "    text_data = []\n",
    "    for text in get_texts_from_resp(resp):\n",
    "        text_data.append(text)\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Searcher():\n",
    "    def __init__(self, w2v=None):\n",
    "        # load w2v modle\n",
    "        if w2v is None:\n",
    "            print(\"start loading w2v, this might take a while\")\n",
    "            self._w2v = KeyedVectors.load_word2vec_format(\"../w2v/model/wiki-news-300d-1M.vec\")\n",
    "        else:\n",
    "            self._w2v = w2v\n",
    "        \n",
    "        # get and process database\n",
    "        try:\n",
    "            self._database = pd.read_csv('crawled_database.csv').iloc[:, [1, 2, 3]]\n",
    "            print(\"load crawled database successful\")\n",
    "        except:\n",
    "            # if no crawled database given\n",
    "            # load the dataset : including only each company's name, url and summary\n",
    "            print('fail to load crawled database')\n",
    "            self._database = pd.read_csv(\"../input/InvestData_2017-Nov-22_0101.csv\").iloc[:, [1, 5, 6]]\n",
    "            self.crawl_database()\n",
    "        self.process_database()\n",
    "        \n",
    "    def process_database(self):\n",
    "        # 1: company name, 5: company website, 6: company manual desc\n",
    "        raw_texts = []\n",
    "        # preprocess all the text data and remove any row without any useful data, and segment each word\n",
    "        drop_list = []\n",
    "        for row in self._database.itertuples():\n",
    "            if not type(row[1]) is str or (not type(row[2]) is str and not type(row[3]) is str):\n",
    "                # check if the row has data\n",
    "                drop_list.append(row[0])\n",
    "            else:\n",
    "                # process text data of both manually summarized or crawled data\n",
    "                tmp_text = []\n",
    "                for col in [2, 3]:\n",
    "                    real_col = col - 1\n",
    "                    text = row[col]\n",
    "                    if type(text) is str:\n",
    "                        text = self.word_tokenize_string(text)\n",
    "                        self._database.iloc[row[0], real_col] = text\n",
    "                        tmp_text.append(text)\n",
    "                # merge texts of same company\n",
    "                tmp_text = '    '.join(tmp_text)\n",
    "                raw_texts.append(tmp_text)\n",
    "                \n",
    "        # drop all the rows that do not have essential data\n",
    "        self._database.drop(drop_list, inplace=True)\n",
    "        # create similarity col for similarity search use\n",
    "        self._database = self._database.assign(similarity=np.zeros(len(self._database)))\n",
    "        \n",
    "        # use the raw_texts to generate tfidf model\n",
    "        self._tfidf, self._dictionary = self.get_tfidf_and_dictionary(raw_texts)\n",
    "        \n",
    "    def crawl_database(self):\n",
    "        for row in self._database.itertuples():\n",
    "            if not (not type(row[1]) is str or (not type(row[2]) is str and not type(row[3]) is str)):\n",
    "                # process each website and replace web address with texts crawled\n",
    "                url = row[2]\n",
    "                texts = self.get_text_from_url_and_its_children(url)\n",
    "                if not texts:\n",
    "                    # if cannot access url, replace url with Nan\n",
    "                    self._database.iloc[row[0], 1] = np.nan\n",
    "                else:\n",
    "                    # replace the url with the crawled texts\n",
    "                    texts = '   '.join(texts)\n",
    "                    self._database.iloc[row[0], 1] = texts\n",
    "        \n",
    "    def save_database(self):\n",
    "        self._database.to_csv('crawled_database.csv')\n",
    "        print(\"database save successful\")\n",
    "    \n",
    "    def update_similarity(self, input_text, col=2):\n",
    "        # get input text vector\n",
    "        input_text_vector = self.get_doc_vector(input_text)\n",
    "        i = 0\n",
    "        for row in self._database.itertuples():\n",
    "            row_text_vector = self.get_doc_vector(row[col])\n",
    "            similarity = input_text_vector.dot(row_text_vector)\n",
    "            self._database.iloc[i, -1] = similarity\n",
    "            i += 1\n",
    "        self._database = self._database.sort_values(by='similarity', ascending=False)\n",
    "        return self._database\n",
    "\n",
    "    def get_doc_vector(self, text):\n",
    "        if not text == text:\n",
    "            return self._w2v['happy'] * 0\n",
    "        tokens = list(self._dictionary.token2id)\n",
    "        # convert any unknown word to known word\n",
    "        new_text = []\n",
    "        for word in text.split():\n",
    "            if word in tokens:\n",
    "                new_text.append(word)\n",
    "            elif word in w2v: # replace the unknow word with the most similar word in tokens of dictionary\n",
    "                new_text.append(self._w2v.most_similar_to_given(word_list=tokens, w1=word))\n",
    "\n",
    "        # start to calculate vector using tfidf weighted word vector sum\n",
    "        # get tfidf weight\n",
    "        tokenized_text = [self._dictionary.doc2bow(new_text)]\n",
    "        tfidf_text = self._tfidf[tokenized_text][0]\n",
    "        # sum weighted word vectors\n",
    "        sum_vector = self._w2v['happy'] * 0 # get the size of the word vector\n",
    "        for word_id, weight in tfidf_text:\n",
    "            word = self._dictionary[word_id]\n",
    "            sum_vector += self._w2v[word] * weight\n",
    "        if sum_vector.any():\n",
    "            sum_vector /= np.sqrt(sum_vector.dot(sum_vector)) # normalize the vector\n",
    "            \n",
    "        return sum_vector\n",
    "    \n",
    "    def word_tokenize_string(self, text):\n",
    "        stop_words = get_stop_words('en') # get too frequent word\n",
    "        text = text.replace('\\r', ' ').replace('\\n', ' ') # remove symbols\n",
    "        text = re.sub(r\"http\\S+\", \"\", text) # remove urls\n",
    "        # remove any word that present too frequently or cannot be converted to word vector\n",
    "        text = [word for sent in sent_tokenize(text.lower()) for word in word_tokenize(sent) \\\n",
    "                if not word in stop_words and word in self._w2v]\n",
    "        return ' '.join(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tfidf_and_dictionary(texts):\n",
    "        # get dictionary of texts\n",
    "        texts = [text.split() for text in texts]\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "        # get tfidf ranking model\n",
    "        tokenized_texts = [dictionary.doc2bow(text) for text in texts]\n",
    "        tfidf = models.TfidfModel(tokenized_texts)\n",
    "\n",
    "        return tfidf, dictionary\n",
    "    \n",
    "    def get_text_from_url_and_its_children(self, main_url):\n",
    "        print(\"starting to crawl main url: \", main_url)\n",
    "        # check validity of main_url\n",
    "        resp = url_is_valid(main_url)\n",
    "        if not resp:\n",
    "            print(\"main_url is not valid\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\nstarting to crawl all its children\")\n",
    "        # grab all urls in this web page\n",
    "        urls = [main_url]\n",
    "        urls.extend(get_urls_from_url(main_url))\n",
    "        urls = list(set(urls)) # remove duplicated urls\n",
    "        print(\"\\n\\nthese are the children links we crawled\")\n",
    "        print(urls, \"\\n\")\n",
    "        # grab all texts in each urls asynchronously\n",
    "        # argmumentize urls\n",
    "        urls = [(url, main_url) for url in urls]\n",
    "        with multiprocessing.Pool(processes=24) as pool:\n",
    "            text_data = pool.starmap(get_text_from_url_with_check, urls) \n",
    "            # try terminating hung jobs\n",
    "        text_data = [text for text in text_data if len(text_data) > 0] # remove empty returns\n",
    "        text_data = [text for text_list in text_data for text in text_list] # get list elements to str\n",
    "        return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load crawled database successful\n"
     ]
    }
   ],
   "source": [
    "searcher = Searcher(w2v=w2v)\n",
    "# there might be main_url + url is not valid, because url and main_url has overlaps, or main_url is not the root\n",
    "# must provide root url, or \n",
    "# use overlaps to do intelligent main_url + url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T19:21:48.699686Z",
     "start_time": "2018-01-22T19:21:48.696223Z"
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"new start up aiming at low income customers, dedicated in green energy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T19:24:06.347593Z",
     "start_time": "2018-01-22T19:24:06.173975Z"
    }
   },
   "outputs": [],
   "source": [
    "%time searcher.update_similarity(input_text, col=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1410px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
